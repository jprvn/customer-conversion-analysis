{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime # For time-based features\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import joblib # To save scaler/encoder\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original cleaned clickstream data loaded: (132379, 14)\n",
      "\n",
      "Aggregating data to session level...\n",
      "Session data recreated: (22910, 17)\n",
      "            session_length  unique_main_categories  most_freq_main_category  \\\n",
      "session_id                                                                    \n",
      "1                        9                       4                        2   \n",
      "2                       10                       3                        2   \n",
      "3                        5                       2                        3   \n",
      "4                        4                       2                        1   \n",
      "5                        1                       1                        3   \n",
      "\n",
      "            unique_models_viewed  unique_colours  most_freq_colour  \\\n",
      "session_id                                                           \n",
      "1                              8               6                 6   \n",
      "2                              8               5                 3   \n",
      "3                              3               3                 6   \n",
      "4                              4               2                 2   \n",
      "5                              1               1                14   \n",
      "\n",
      "            avg_price_viewed  session_revenue_potential  max_price_viewed  \\\n",
      "session_id                                                                  \n",
      "1                  43.875000                      351.0              57.0   \n",
      "2                  49.222222                      443.0              67.0   \n",
      "3                  44.666667                      134.0              48.0   \n",
      "4                  45.250000                      181.0              62.0   \n",
      "5                  57.000000                       57.0              57.0   \n",
      "\n",
      "            max_page_reached  total_pages_viewed  month_first  day_first  \\\n",
      "session_id                                                                 \n",
      "1                          5                   8            4          1   \n",
      "2                          2                   9            4          1   \n",
      "3                          1                   3            4          1   \n",
      "4                          3                   4            4          1   \n",
      "5                          2                   1            4          1   \n",
      "\n",
      "            country_first  model_photography_<lambda>  price_2_<lambda>  \\\n",
      "session_id                                                                \n",
      "1                      29                           2                 1   \n",
      "2                      29                           1                 1   \n",
      "3                      21                           1                 1   \n",
      "4                      21                           1                 1   \n",
      "5                       9                           1                 1   \n",
      "\n",
      "            purchase_completed  \n",
      "session_id                      \n",
      "1                            1  \n",
      "2                            0  \n",
      "3                            0  \n",
      "4                            0  \n",
      "5                            0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22910 entries, 1 to 24026\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   session_length              22910 non-null  int64  \n",
      " 1   unique_main_categories      22910 non-null  int64  \n",
      " 2   most_freq_main_category     22910 non-null  int64  \n",
      " 3   unique_models_viewed        22910 non-null  int64  \n",
      " 4   unique_colours              22910 non-null  int64  \n",
      " 5   most_freq_colour            22910 non-null  int64  \n",
      " 6   avg_price_viewed            22910 non-null  float64\n",
      " 7   session_revenue_potential   22910 non-null  float64\n",
      " 8   max_price_viewed            22910 non-null  float64\n",
      " 9   max_page_reached            22910 non-null  int64  \n",
      " 10  total_pages_viewed          22910 non-null  int64  \n",
      " 11  month_first                 22910 non-null  int64  \n",
      " 12  day_first                   22910 non-null  int64  \n",
      " 13  country_first               22910 non-null  int64  \n",
      " 14  model_photography_<lambda>  22910 non-null  int64  \n",
      " 15  price_2_<lambda>            22910 non-null  int64  \n",
      " 16  purchase_completed          22910 non-null  int64  \n",
      "dtypes: float64(3), int64(14)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Load Original Cleaned Clickstream Data ---\n",
    "processed_data_path = 'E:/GUVI/Projects/customer_conversion_analysis/data_processed/train_cleaned.csv'\n",
    "try:\n",
    "    df_train_clicks = pd.read_csv(processed_data_path)\n",
    "    print(f\"Original cleaned clickstream data loaded: {df_train_clicks.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {processed_data_path}\")\n",
    "    raise\n",
    "\n",
    "# --- Re-run Aggregation Logic ---\n",
    "print(\"\\nAggregating data to session level...\")\n",
    "aggregation_functions = {\n",
    "    'order': 'max',\n",
    "    'page_1_main_category': ['nunique', lambda x: x.mode()[0] if not x.mode().empty else None],\n",
    "    'page_2_clothing_model': 'nunique',\n",
    "    'colour': ['nunique', lambda x: x.mode()[0] if not x.mode().empty else None],\n",
    "    'price': ['mean', 'sum', 'max'],\n",
    "    'page': ['max', 'count'],\n",
    "    'month': 'first',\n",
    "    'day': 'first',\n",
    "    'country': 'first', # Assuming country is consistent per session\n",
    "     # Keep necessary fields for feature engineering\n",
    "    'model_photography': lambda x: x.mode()[0] if not x.mode().empty else None, # Most common photo type\n",
    "    'price_2': lambda x: x.mode()[0] if not x.mode().empty else None # Most common price category indicator\n",
    "}\n",
    "\n",
    "df_session = df_train_clicks.groupby('session_id').agg(aggregation_functions)\n",
    "\n",
    "# Rename columns\n",
    "df_session.columns = ['_'.join(map(str, col)).strip('_') for col in df_session.columns.values] # Ensure col names are strings\n",
    "df_session = df_session.rename(columns={\n",
    "    'order_max': 'session_length',\n",
    "    'page_1_main_category_nunique': 'unique_main_categories',\n",
    "    'page_1_main_category_<lambda_0>': 'most_freq_main_category',\n",
    "    'page_2_clothing_model_nunique': 'unique_models_viewed',\n",
    "    'colour_nunique': 'unique_colours',\n",
    "    'colour_<lambda_0>': 'most_freq_colour',\n",
    "    'price_mean': 'avg_price_viewed',\n",
    "    'price_sum': 'session_revenue_potential', # Regression Target\n",
    "    'price_max': 'max_price_viewed',\n",
    "    'page_max': 'max_page_reached',\n",
    "    'page_count': 'total_pages_viewed', # Often same as session_length\n",
    "    'model_photography_<lambda_0>': 'most_freq_model_photo',\n",
    "    'price_2_<lambda_0>': 'most_freq_price_indicator'\n",
    "    # Keep month_first, day_first, country_first for now\n",
    "})\n",
    "\n",
    "# Define Classification Target\n",
    "df_session['purchase_completed'] = (df_session['max_page_reached'] == 5).astype(int)\n",
    "\n",
    "print(f\"Session data recreated: {df_session.shape}\")\n",
    "print(df_session.head())\n",
    "df_session.info() # Check dtypes, especially for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dropping Redundant/Unnecessary Features ---\n",
      "Columns dropped: ['month_first', 'day_first', 'date', 'max_page_reached', 'total_pages_viewed']\n",
      "Shape after dropping: (22910, 13)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22910 entries, 1 to 24026\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   session_length              22910 non-null  int64  \n",
      " 1   unique_main_categories      22910 non-null  int64  \n",
      " 2   most_freq_main_category     22910 non-null  int64  \n",
      " 3   unique_models_viewed        22910 non-null  int64  \n",
      " 4   unique_colours              22910 non-null  int64  \n",
      " 5   most_freq_colour            22910 non-null  int64  \n",
      " 6   avg_price_viewed            22910 non-null  float64\n",
      " 7   session_revenue_potential   22910 non-null  float64\n",
      " 8   max_price_viewed            22910 non-null  float64\n",
      " 9   country_first               22910 non-null  int64  \n",
      " 10  model_photography_<lambda>  22910 non-null  int64  \n",
      " 11  price_2_<lambda>            22910 non-null  int64  \n",
      " 12  purchase_completed          22910 non-null  int64  \n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 2.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Dropping Redundant/Unnecessary Features ---\")\n",
    "columns_to_drop = [\n",
    "    'month_first',                  # Used to create date features\n",
    "    'day_first',                    # Used to create date features\n",
    "    'date',                         # Intermediate date column\n",
    "    'max_page_reached',             # Used to create target, potential leakage\n",
    "    'total_pages_viewed',           # Likely redundant with session_length\n",
    "    # Consider dropping features with too many unique values if not handled\n",
    "    # 'page_2_clothing_model_nunique' # Example - decide based on EDA/utility\n",
    "]\n",
    "# Drop columns only if they exist in the dataframe\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df_session.columns]\n",
    "df_session_fe = df_session.drop(columns=existing_columns_to_drop)\n",
    "\n",
    "print(f\"Columns dropped: {existing_columns_to_drop}\")\n",
    "print(f\"Shape after dropping: {df_session_fe.shape}\")\n",
    "print(df_session_fe.info()) # Check remaining columns and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Identifying Feature Types for Preprocessing ---\n",
      "Numerical Features (8): ['session_length', 'unique_main_categories', 'unique_models_viewed', 'unique_colours', 'avg_price_viewed', 'max_price_viewed', 'model_photography_<lambda>', 'price_2_<lambda>']\n",
      "Categorical Features (3): ['country_first', 'most_freq_main_category', 'most_freq_colour']\n",
      "\n",
      "Features DataFrame X shape: (22910, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Identifying Feature Types for Preprocessing ---\")\n",
    "\n",
    "# Separate Targets\n",
    "y_class = df_session_fe['purchase_completed']\n",
    "y_reg = df_session_fe['session_revenue_potential']\n",
    "X = df_session_fe.drop(columns=['purchase_completed', 'session_revenue_potential'])\n",
    "\n",
    "# Identify numerical and categorical features IN THE REMAINING X DATAFRAME\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist() # Check if any objects remain\n",
    "\n",
    "# MANUALLY review and adjust the lists if needed.\n",
    "# Features like 'country', 'most_freq_main_category', 'most_freq_colour' should be treated as categorical even if numeric\n",
    "potential_cats = ['country_first', 'most_freq_main_category', 'most_freq_colour',\n",
    "                 'most_freq_model_photo', 'most_freq_price_indicator',\n",
    "                 'day_of_week', 'is_weekend'] # Treat day_of_week as categorical? Or keep numeric? Let's try categorical.\n",
    "\n",
    "# Refine lists\n",
    "categorical_features = [col for col in potential_cats if col in X.columns]\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "\n",
    "print(f\"Numerical Features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\nFeatures DataFrame X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Preprocessing Pipeline ---\n",
      "Fitting and transforming data with the preprocessor...\n",
      "\n",
      "Total features after processing: 72\n",
      "\n",
      "--- Processed Data Head ---\n",
      "            session_length  unique_main_categories  unique_models_viewed  \\\n",
      "session_id                                                                 \n",
      "1                 0.041237                1.000000              0.051852   \n",
      "2                 0.046392                0.666667              0.051852   \n",
      "3                 0.020619                0.333333              0.014815   \n",
      "4                 0.015464                0.333333              0.022222   \n",
      "5                 0.000000                0.000000              0.000000   \n",
      "\n",
      "            unique_colours  avg_price_viewed  max_price_viewed  \\\n",
      "session_id                                                       \n",
      "1                 0.384615          0.404297          0.609375   \n",
      "2                 0.307692          0.487847          0.765625   \n",
      "3                 0.153846          0.416667          0.468750   \n",
      "4                 0.076923          0.425781          0.687500   \n",
      "5                 0.000000          0.609375          0.609375   \n",
      "\n",
      "            model_photography_<lambda>  price_2_<lambda>  country_first_1  \\\n",
      "session_id                                                                  \n",
      "1                                  1.0               0.0              0.0   \n",
      "2                                  0.0               0.0              0.0   \n",
      "3                                  0.0               0.0              0.0   \n",
      "4                                  0.0               0.0              0.0   \n",
      "5                                  0.0               0.0              0.0   \n",
      "\n",
      "            country_first_2  ...  most_freq_colour_5  most_freq_colour_6  \\\n",
      "session_id                   ...                                           \n",
      "1                       0.0  ...                 0.0                 1.0   \n",
      "2                       0.0  ...                 0.0                 0.0   \n",
      "3                       0.0  ...                 0.0                 1.0   \n",
      "4                       0.0  ...                 0.0                 0.0   \n",
      "5                       0.0  ...                 0.0                 0.0   \n",
      "\n",
      "            most_freq_colour_7  most_freq_colour_8  most_freq_colour_9  \\\n",
      "session_id                                                               \n",
      "1                          0.0                 0.0                 0.0   \n",
      "2                          0.0                 0.0                 0.0   \n",
      "3                          0.0                 0.0                 0.0   \n",
      "4                          0.0                 0.0                 0.0   \n",
      "5                          0.0                 0.0                 0.0   \n",
      "\n",
      "            most_freq_colour_10  most_freq_colour_11  most_freq_colour_12  \\\n",
      "session_id                                                                  \n",
      "1                           0.0                  0.0                  0.0   \n",
      "2                           0.0                  0.0                  0.0   \n",
      "3                           0.0                  0.0                  0.0   \n",
      "4                           0.0                  0.0                  0.0   \n",
      "5                           0.0                  0.0                  0.0   \n",
      "\n",
      "            most_freq_colour_13  most_freq_colour_14  \n",
      "session_id                                            \n",
      "1                           0.0                  0.0  \n",
      "2                           0.0                  0.0  \n",
      "3                           0.0                  0.0  \n",
      "4                           0.0                  0.0  \n",
      "5                           0.0                  1.0  \n",
      "\n",
      "[5 rows x 72 columns]\n",
      "\n",
      "--- Processed Data Description ---\n",
      "       session_length  unique_main_categories  unique_models_viewed  \\\n",
      "count    22910.000000            22910.000000          22910.000000   \n",
      "mean         0.030837                0.265081              0.031634   \n",
      "std          0.046881                0.308498              0.046396   \n",
      "min          0.000000                0.000000              0.000000   \n",
      "25%          0.005155                0.000000              0.000000   \n",
      "50%          0.015464                0.333333              0.014815   \n",
      "75%          0.041237                0.333333              0.044444   \n",
      "max          1.000000                1.000000              1.000000   \n",
      "\n",
      "       unique_colours  avg_price_viewed  max_price_viewed  \\\n",
      "count    22910.000000      22910.000000      22910.000000   \n",
      "mean         0.173874          0.412016          0.577348   \n",
      "std          0.185171          0.137068          0.207582   \n",
      "min          0.000000          0.000000          0.000000   \n",
      "25%          0.000000          0.312500          0.390625   \n",
      "50%          0.153846          0.397905          0.609375   \n",
      "75%          0.307692          0.489583          0.687500   \n",
      "max          1.000000          1.000000          1.000000   \n",
      "\n",
      "       model_photography_<lambda>  price_2_<lambda>  country_first_1  \\\n",
      "count                22910.000000      22910.000000     22910.000000   \n",
      "mean                     0.134003          0.405762         0.000044   \n",
      "std                      0.340663          0.491050         0.006607   \n",
      "min                      0.000000          0.000000         0.000000   \n",
      "25%                      0.000000          0.000000         0.000000   \n",
      "50%                      0.000000          0.000000         0.000000   \n",
      "75%                      0.000000          1.000000         0.000000   \n",
      "max                      1.000000          1.000000         1.000000   \n",
      "\n",
      "       country_first_2  ...  most_freq_colour_5  most_freq_colour_6  \\\n",
      "count     22910.000000  ...        22910.000000        22910.000000   \n",
      "mean          0.000480  ...            0.004670            0.087866   \n",
      "std           0.021907  ...            0.068182            0.283105   \n",
      "min           0.000000  ...            0.000000            0.000000   \n",
      "25%           0.000000  ...            0.000000            0.000000   \n",
      "50%           0.000000  ...            0.000000            0.000000   \n",
      "75%           0.000000  ...            0.000000            0.000000   \n",
      "max           1.000000  ...            1.000000            1.000000   \n",
      "\n",
      "       most_freq_colour_7  most_freq_colour_8  most_freq_colour_9  \\\n",
      "count        22910.000000        22910.000000        22910.000000   \n",
      "mean             0.027193            0.008904            0.063204   \n",
      "std              0.162650            0.093944            0.243335   \n",
      "min              0.000000            0.000000            0.000000   \n",
      "25%              0.000000            0.000000            0.000000   \n",
      "50%              0.000000            0.000000            0.000000   \n",
      "75%              0.000000            0.000000            0.000000   \n",
      "max              1.000000            1.000000            1.000000   \n",
      "\n",
      "       most_freq_colour_10  most_freq_colour_11  most_freq_colour_12  \\\n",
      "count         22910.000000         22910.000000         22910.000000   \n",
      "mean              0.012615             0.015714             0.030816   \n",
      "std               0.111606             0.124368             0.172823   \n",
      "min               0.000000             0.000000             0.000000   \n",
      "25%               0.000000             0.000000             0.000000   \n",
      "50%               0.000000             0.000000             0.000000   \n",
      "75%               0.000000             0.000000             0.000000   \n",
      "max               1.000000             1.000000             1.000000   \n",
      "\n",
      "       most_freq_colour_13  most_freq_colour_14  \n",
      "count         22910.000000         22910.000000  \n",
      "mean              0.019729             0.061632  \n",
      "std               0.139072             0.240492  \n",
      "min               0.000000             0.000000  \n",
      "25%               0.000000             0.000000  \n",
      "50%               0.000000             0.000000  \n",
      "75%               0.000000             0.000000  \n",
      "max               1.000000             1.000000  \n",
      "\n",
      "[8 rows x 72 columns]\n",
      "\n",
      "Shape of processed features X: (22910, 72)\n",
      "Shape of classification target y_class: (22910,)\n",
      "Shape of regression target y_reg: (22910,)\n",
      "\n",
      "Preprocessor saved to: ../src/models\\preprocessor.joblib\n",
      "Feature names saved to: ../src/models\\feature_names.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Setting up Preprocessing Pipeline ---\")\n",
    "\n",
    "# Define preprocessing steps for numerical and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()) # Or StandardScaler()\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # handle_unknown='ignore' is important for test set\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "# Use remainder='passthrough' if there are columns you want to keep unchanged (unlikely here)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop' # Drop any columns not specified\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to the features X\n",
    "print(\"Fitting and transforming data with the preprocessor...\")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get feature names after OneHotEncoding\n",
    "# This is important for interpreting feature importances later\n",
    "try:\n",
    "    ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = numerical_features + list(ohe_feature_names)\n",
    "    print(f\"\\nTotal features after processing: {len(all_feature_names)}\")\n",
    "\n",
    "    # Convert the processed numpy array back to a DataFrame (optional but good for inspection)\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "    print(\"\\n--- Processed Data Head ---\")\n",
    "    print(X_processed_df.head())\n",
    "\n",
    "    print(\"\\n--- Processed Data Description ---\")\n",
    "    print(X_processed_df.describe())\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Warning: Could not retrieve feature names automatically. Check scikit-learn version.\")\n",
    "    # Handle older versions or alternative ways if needed\n",
    "    X_processed_df = pd.DataFrame(X_processed, index=X.index) # Create DF without column names\n",
    "\n",
    "print(f\"\\nShape of processed features X: {X_processed.shape}\")\n",
    "print(f\"Shape of classification target y_class: {y_class.shape}\")\n",
    "print(f\"Shape of regression target y_reg: {y_reg.shape}\")\n",
    "\n",
    "# --- Save the Preprocessor and Feature Names ---\n",
    "output_dir = '../src/models' # Define directory to save artifacts\n",
    "os.makedirs(output_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "preprocessor_path = os.path.join(output_dir, 'preprocessor.joblib')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"\\nPreprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Save feature names (important for consistency)\n",
    "feature_names_path = os.path.join(output_dir, 'feature_names.joblib')\n",
    "joblib.dump(all_feature_names, feature_names_path)\n",
    "print(f\"Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# --- Save Processed Data (Optional but can be useful) ---\n",
    "# You can save X_processed_df, y_class, y_reg if needed for separate modeling scripts\n",
    "# processed_data_output_path = '../src/data/processed/train_final_features.csv'\n",
    "# X_processed_df.to_csv(processed_data_output_path, index=False)\n",
    "# y_class.to_csv('../src/data/processed/train_final_y_class.csv', index=False, header=True)\n",
    "# y_reg.to_csv('../src/data/processed/train_final_y_reg.csv', index=False, header=True)\n",
    "# print(\"Processed data frames saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Engineering Time-Based Features ---\n",
      "Day of week and weekend features created.\n",
      "                 date  day_of_week  is_weekend\n",
      "session_id                                    \n",
      "1          2008-04-01            1           1\n",
      "2          2008-04-01            1           1\n",
      "3          2008-04-01            1           1\n",
      "4          2008-04-01            1           1\n",
      "5          2008-04-01            1           1\n",
      "\n",
      "Checking dtypes after date conversion:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22910 entries, 1 to 24026\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date         22910 non-null  datetime64[ns]\n",
      " 1   day_of_week  22910 non-null  Int64         \n",
      " 2   is_weekend   22910 non-null  Int64         \n",
      "dtypes: Int64(2), datetime64[ns](1)\n",
      "memory usage: 760.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Engineering Time-Based Features ---\")\n",
    "# We know the year is 2008\n",
    "# Create a date column (handle potential errors if day/month invalid - unlikely here)\n",
    "try:\n",
    "    # Original line likely causing issues if apply returns mixed types or errors happen silently\n",
    "    # df_session['date'] = df_session.apply(lambda row: datetime.date(2008, int(row['month_first']), int(row['day_first'])), axis=1)\n",
    "\n",
    "    # --- CORRECTED APPROACH using pd.to_datetime ---\n",
    "    # Create string representation first 'YYYY-MM-DD'\n",
    "    df_session['date_str'] = '2008-' + df_session['month_first'].astype(str) + '-' + df_session['day_first'].astype(str)\n",
    "    # Convert string column to datetime objects\n",
    "    df_session['date'] = pd.to_datetime(df_session['date_str'], format='%Y-%m-%d', errors='coerce')\n",
    "    # Check for any dates that failed conversion (became NaT - Not a Time)\n",
    "    if df_session['date'].isnull().any():\n",
    "        print(\"Warning: Some date conversions failed. Review 'month_first' and 'day_first'. Rows with NaT:\")\n",
    "        print(df_session[df_session['date'].isnull()][['month_first', 'day_first']])\n",
    "        # Decide how to handle NaT: fillna, drop rows, etc. For now, we proceed but be aware.\n",
    "\n",
    "    # Drop the intermediate string column\n",
    "    df_session = df_session.drop(columns=['date_str'])\n",
    "\n",
    "    # Now, this part should work if conversion was successful\n",
    "    df_session['day_of_week'] = df_session['date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df_session['is_weekend'] = df_session['day_of_week'].apply(lambda x: 1 if pd.notnull(x) else x).astype('Int64') # Handle potential NaT from conversion, use nullable Int\n",
    "    # Convert day_of_week to nullable Int as well if you handled NaT\n",
    "    df_session['day_of_week'] = df_session['day_of_week'].astype('Int64')\n",
    "\n",
    "\n",
    "    print(\"Day of week and weekend features created.\")\n",
    "    print(df_session[['date', 'day_of_week', 'is_weekend']].head())\n",
    "    # Check dtypes again\n",
    "    print(\"\\nChecking dtypes after date conversion:\")\n",
    "    print(df_session[['date', 'day_of_week', 'is_weekend']].info())\n",
    "\n",
    "except Exception as e: # Catch more general exceptions during conversion\n",
    "     print(f\"Error creating date features: {e}. Check month/day values.\")\n",
    "     # Handle error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessor saved to: models\\preprocessor.joblib\n",
      "Feature names saved to: models\\feature_names.joblib\n",
      "Processed features saved to: data_processed\\train_final_features.csv\n",
      "Classification target saved to: data_processed\\train_final_y_class.csv\n",
      "Regression target saved to: data_processed\\train_final_y_reg.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Preprocessor and Feature Names ---\n",
    "output_dir = 'models' # Define directory to save artifacts\n",
    "os.makedirs(output_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "preprocessor_path = os.path.join(output_dir, 'preprocessor.joblib')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"\\nPreprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Save feature names (important for consistency)\n",
    "feature_names_path = os.path.join(output_dir, 'feature_names.joblib')\n",
    "joblib.dump(all_feature_names, feature_names_path)\n",
    "print(f\"Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# --- Save Processed Data (Optional but can be useful) ---\n",
    "# This part was commented out before - let's uncomment and ensure paths are correct\n",
    "processed_data_output_dir = 'data_processed' # Define directory for processed data output\n",
    "os.makedirs(processed_data_output_dir, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "processed_features_path = os.path.join(processed_data_output_dir, 'train_final_features.csv')\n",
    "y_class_path = os.path.join(processed_data_output_dir, 'train_final_y_class.csv')\n",
    "y_reg_path = os.path.join(processed_data_output_dir, 'train_final_y_reg.csv')\n",
    "\n",
    "# Ensure X_processed_df exists (it's created earlier in the try block)\n",
    "if 'X_processed_df' in locals():\n",
    "        X_processed_df.to_csv(processed_features_path, index=False)\n",
    "        print(f\"Processed features saved to: {processed_features_path}\")\n",
    "else:\n",
    "        print(\"Warning: X_processed_df not found. Cannot save features.\")\n",
    "\n",
    "# Ensure y_class and y_reg exist (defined earlier)\n",
    "if 'y_class' in locals():\n",
    "        y_class.to_csv(y_class_path, index=False, header=True) # Save header for clarity\n",
    "        print(f\"Classification target saved to: {y_class_path}\")\n",
    "else:\n",
    "        print(\"Warning: y_class not found. Cannot save target.\")\n",
    "\n",
    "if 'y_reg' in locals():\n",
    "        y_reg.to_csv(y_reg_path, index=False, header=True) # Save header for clarity\n",
    "        print(f\"Regression target saved to: {y_reg_path}\")\n",
    "else:\n",
    "    print(\"Warning: y_reg not found. Cannot save target.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
